{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Compose, functional\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df, input_size, pred_type, batch_size):\n",
    " \n",
    "    if pred_type == 'low':\n",
    "        df = df[:-2]\n",
    "        \n",
    "    values, last_close = adjust_prev_close(df, pred_type)\n",
    "    \n",
    "    if len(values) % input_size != 0:\n",
    "        size = len(values) // input_size * input_size\n",
    "        values = values[-size:]\n",
    "    \n",
    "    transform = StandardScaler().fit(values.reshape(-1,1))\n",
    "    values = transform.transform(values.reshape(-1,1))[:,0]\n",
    "    values = torch.from_numpy(values).float()\n",
    "\n",
    "    len_dataset = (len(df)-input_size -1)//batch_size * batch_size \n",
    "\n",
    "    dataset = torch.zeros(len_dataset,input_size)\n",
    "    dataset[-1] = values[-input_size:]\n",
    "\n",
    "    for i in range(2,len_dataset+1):\n",
    "        dataset[-i] = values[-(i+input_size-1):-(i-1)]\n",
    "\n",
    "    predset = dataset[-1].unsqueeze(0)\n",
    "\n",
    "    return dataset, predset, {'transform':transform,'last_close':last_close}\n",
    "    \n",
    "def adjust_prev_close(df,pred_type):\n",
    "    last_close = df.iloc[-1]['close']\n",
    "    prev_close = df['close'].shift(fill_value=0).values[1:]\n",
    "    column = df[pred_type].values[1:]\n",
    "    result = (column - prev_close)/prev_close\n",
    "    return result, last_close\n",
    "\n",
    "def get_data(df,input_size, batch_size):\n",
    "    data, dl, tf = defaultdict(dict), defaultdict(dict), {}\n",
    "    \n",
    "    for pred_type in ['low', 'high']:\n",
    "        train_data,pred_input,transform = process_data(df, input_size, pred_type, batch_size)\n",
    "        data[pred_type]['train'] = torch.utils.data.TensorDataset(train_data)\n",
    "        data[pred_type]['pred'] = torch.utils.data.TensorDataset(pred_input)\n",
    "        dl[pred_type]['train'] = DataLoader(data[pred_type]['train'], batch_size=batch_size, shuffle=True, num_workers=0)   \n",
    "        dl[pred_type]['val'] = DataLoader(data[pred_type]['train'], batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "        tf[pred_type] = transform\n",
    "    return dl, tf\n",
    "\n",
    "def get_all_data(input_size, filepath, batch_size):\n",
    "    datasets, dataloaders, transforms = defaultdict(dict), defaultdict(dict), defaultdict(dict)\n",
    "    d_all = pd.read_pickle(filepath)\n",
    "    for ticker, dates in d_all.items():    \n",
    "        for date,df in dates.items():\n",
    "            print(ticker,date)\n",
    "            try:\n",
    "                dataloaders[ticker][date]\\\n",
    "                , transforms[ticker][date] = get_data(df,input_size, batch_size)\n",
    "            except Exception as err:\n",
    "                print(ticker,date,err)\n",
    "    return dataloaders, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MaskedLinear(nn.Linear):\n",
    "    \"\"\"Masked linear layer for MADE: takes in mask as input and masks out connections in the linear layers.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size, mask):\n",
    "        super().__init__(input_size, output_size)\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.linear(x, self.mask * self.weight, self.bias)\n",
    "\n",
    "\n",
    "class PermuteLayer(nn.Module):\n",
    "    \"\"\"Layer to permute the ordering of inputs.\n",
    "\n",
    "    Because our data is 2-D, forward() and inverse() will reorder the data in the same way.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs):\n",
    "        super(PermuteLayer, self).__init__()\n",
    "        self.perm = np.array(np.arange(0, num_inputs)[::-1])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return inputs[:, self.perm], torch.zeros(\n",
    "            inputs.size(0), 1, device=inputs.device\n",
    "        )\n",
    "\n",
    "    def inverse(self, inputs):\n",
    "        return inputs[:, self.perm], torch.zeros(\n",
    "            inputs.size(0), 1, device=inputs.device\n",
    "        )\n",
    "\n",
    "\n",
    "class MADE(nn.Module):\n",
    "    \"\"\"Masked Autoencoder for Distribution Estimation.\n",
    "    https://arxiv.org/abs/1502.03509\n",
    "\n",
    "    Uses sequential ordering as in the MAF paper.\n",
    "    Gaussian MADE to work with real-valued inputs\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, n_hidden):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        masks = self.create_masks()\n",
    "\n",
    "        # construct layers: inner, hidden(s), output\n",
    "        self.net = [MaskedLinear(self.input_size, self.hidden_size, masks[0])]\n",
    "        self.net += [nn.ReLU(inplace=True)]\n",
    "        # iterate over number of hidden layers\n",
    "        for i in range(self.n_hidden):\n",
    "            self.net += [MaskedLinear(self.hidden_size, self.hidden_size, masks[i + 1])]\n",
    "            self.net += [nn.ReLU(inplace=True)]\n",
    "        # last layer doesn't have nonlinear activation\n",
    "        self.net += [\n",
    "            MaskedLinear(self.hidden_size, self.input_size * 2, masks[-1].repeat(2, 1))\n",
    "        ]\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "\n",
    "    def create_masks(self):\n",
    "        \"\"\"\n",
    "        Creates masks for sequential (natural) ordering.\n",
    "        \"\"\"\n",
    "        masks = []\n",
    "        input_degrees = torch.arange(self.input_size)\n",
    "        degrees = [input_degrees]  # corresponds to m(k) in paper\n",
    "\n",
    "        # iterate through every hidden layer\n",
    "        for n_h in range(self.n_hidden + 1):\n",
    "            degrees += [torch.arange(self.hidden_size) % (self.input_size - 1)]\n",
    "        degrees += [input_degrees % self.input_size - 1]\n",
    "        self.m = degrees\n",
    "\n",
    "        # output layer mask\n",
    "        for (d0, d1) in zip(degrees[:-1], degrees[1:]):\n",
    "            masks += [(d1.unsqueeze(-1) >= d0.unsqueeze(0)).float()]\n",
    "\n",
    "        return masks\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Run the forward mapping (z -> x) for MAF through one MADE block.\n",
    "        :param z: Input noise of size (batch_size, self.input_size)\n",
    "        :return: (x, log_det). log_det should be 1-D (batch_dim,)\n",
    "        \"\"\"\n",
    "        x = torch.zeros_like(z)\n",
    "\n",
    "        for i in range(self.input_size):\n",
    "            h = self.net(x)\n",
    "            mu, alpha = torch.split(h, h.size(-1) // 2, dim =-1)\n",
    "            x[:,i] = mu[:,i] + z[:,i] * torch.exp(alpha[:,i])\n",
    "        log_det = -torch.sum(alpha, dim = -1)\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "        return x, log_det\n",
    "\n",
    "    def inverse(self, x):\n",
    "        \"\"\"\n",
    "        Run one inverse mapping (x -> z) for MAF through one MADE block.\n",
    "        :param x: Input data of size (batch_size, self.input_size)\n",
    "        :return: (z, log_det). log_det should be 1-D (batch_dim,)\n",
    "        \"\"\"\n",
    "        # YOUR CODE STARTS HERE\n",
    "        h =  self.net(x)\n",
    "        mu, alpha = torch.split(h, h.size(-1) // 2, dim =-1)\n",
    "        z = (x - mu) / torch.exp(alpha)\n",
    "        log_det = -torch.sum(alpha, dim = -1)\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "        return z, log_det\n",
    "\n",
    "\n",
    "class MAF(nn.Module):\n",
    "    \"\"\"\n",
    "    Masked Autoregressive Flow, using MADE layers.\n",
    "    https://arxiv.org/abs/1705.07057\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, n_hidden, n_flows):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_flows = n_flows\n",
    "        self.base_dist = torch.distributions.normal.Normal(0, 1)\n",
    "\n",
    "        # need to flip ordering of inputs for every layer\n",
    "        nf_blocks = []\n",
    "        for i in range(self.n_flows):\n",
    "            nf_blocks.append(MADE(self.input_size, self.hidden_size, self.n_hidden))\n",
    "            nf_blocks.append(PermuteLayer(self.input_size))  # permute dims\n",
    "        self.nf = nn.Sequential(*nf_blocks)\n",
    "\n",
    "    def log_probs(self, x):\n",
    "        \"\"\"\n",
    "        Obtain log-likelihood p(x) through one pass of MADE\n",
    "        :param x: Input data of size (batch_size, self.input_size)\n",
    "        :return: log_prob. This should be a Python scalar.\n",
    "        \"\"\"\n",
    "        # YOUR CODE STARTS HERE\n",
    "        sum_log_det = torch.zeros(x.size(0),1)\n",
    "        \n",
    "        for flow in self.nf:\n",
    "            x, log_det = flow.inverse(x)\n",
    "            if len(log_det.size()) ==1:\n",
    "                 log_det = log_det.unsqueeze(1)\n",
    "            sum_log_det += log_det\n",
    "\n",
    "        log_prob = torch.mean(torch.sum(self.base_dist.log_prob(x),dim=-1) + sum_log_det)\n",
    "        \n",
    "        return log_prob\n",
    "\n",
    "    def loss(self, x):\n",
    "        \"\"\"\n",
    "        Compute the loss.\n",
    "        :param x: Input data of size (batch_size, self.input_size)\n",
    "        :return: loss. This should be a Python scalar.\n",
    "        \"\"\"\n",
    "        return -self.log_probs(x)\n",
    "\n",
    "    def sample(self, device, n):\n",
    "        \"\"\"\n",
    "        Draw <n> number of samples from the model.\n",
    "        :param device: [cpu,cuda]\n",
    "        :param n: Number of samples to be drawn.\n",
    "        :return: x_sample. This should be a numpy array of size (n, self.input_size)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            x_sample = torch.randn(n, self.input_size).to(device)\n",
    "            for flow in self.nf[::-1]:\n",
    "                x_sample, log_det = flow.forward(x_sample)\n",
    "            x_sample = x_sample.view(n, self.input_size)\n",
    "            x_sample = x_sample.cpu().data.numpy()\n",
    "\n",
    "        return x_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,n_epochs,loader,device,optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    batch_idx = 0.0\n",
    "\n",
    "    for epoch in range(1,n_epochs + 1):\n",
    "        for data in loader:\n",
    "            batch_idx += 1\n",
    "\n",
    "            if isinstance(data, list):\n",
    "                data = data[0]\n",
    "\n",
    "            batch_size = len(data)\n",
    "            data = data.view(batch_size, -1)\n",
    "\n",
    "            data = data.to(device)\n",
    "\n",
    "            # run MAF\n",
    "            loss = model.loss(data)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # save stuff\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        total_loss /= batch_idx + 1\n",
    "        #print(\"Average train log-likelihood: {:.6f}\".format(-total_loss))\n",
    "\n",
    "    return model,optimizer,total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COST 2022-10-27 00:00:00\n",
      "COST 2023-02-02 00:00:00\n",
      "COST 2023-05-04 00:00:00\n",
      "COST 2023-08-24 00:00:00\n",
      "COST 2023-11-02 00:00:00\n",
      "CE 2022-10-28 00:00:00\n",
      "CE 2023-07-28 00:00:00\n",
      "CE 2023-10-27 00:00:00\n",
      "AON 2022-10-31 00:00:00\n",
      "AON 2023-01-31 00:00:00\n",
      "AON 2023-04-28 00:00:00\n",
      "AON 2023-07-31 00:00:00\n",
      "AON 2023-10-31 00:00:00\n",
      "MKTX 2022-11-01 00:00:00\n",
      "MKTX 2023-02-07 00:00:00\n",
      "MKTX 2023-05-09 00:00:00\n",
      "MKTX 2023-08-01 00:00:00\n",
      "CAG 2022-11-02 00:00:00\n",
      "CAG 2023-11-01 00:00:00\n",
      "NSC 2022-11-03 00:00:00\n",
      "NSC 2023-08-03 00:00:00\n",
      "AMP 2022-11-04 00:00:00\n",
      "AMP 2023-02-09 00:00:00\n",
      "AMP 2023-08-04 00:00:00\n",
      "AWK 2022-11-07 00:00:00\n",
      "AWK 2023-02-06 00:00:00\n",
      "AWK 2023-05-08 00:00:00\n",
      "AWK 2023-08-07 00:00:00\n",
      "WST 2022-11-08 00:00:00\n",
      "WST 2023-01-24 00:00:00\n",
      "WST 2023-04-25 00:00:00\n",
      "WST 2023-07-25 00:00:00\n",
      "POOL 2022-11-09 00:00:00\n",
      "POOL 2023-05-16 00:00:00\n",
      "POOL 2023-08-09 00:00:00\n",
      "GWW 2022-11-10 00:00:00\n",
      "GWW 2023-02-10 00:00:00\n",
      "GWW 2023-05-05 00:00:00\n",
      "GWW 2023-08-11 00:00:00\n",
      "TFX 2022-11-14 00:00:00\n",
      "TFX 2023-03-02 00:00:00\n",
      "TFX 2023-08-14 00:00:00\n",
      "EQIX 2022-11-15 00:00:00\n",
      "EQIX 2023-05-23 00:00:00\n",
      "EQIX 2023-08-22 00:00:00\n",
      "AMGN 2022-11-16 00:00:00\n",
      "AMGN 2023-05-17 00:00:00\n",
      "WHR 2022-11-17 00:00:00\n",
      "WHR 2023-05-18 00:00:00\n",
      "TSCO 2022-11-18 00:00:00\n",
      "SWKS 2022-11-21 00:00:00\n",
      "SWKS 2023-08-28 00:00:00\n",
      "EG 2022-11-22 00:00:00\n",
      "EG 2023-03-15 00:00:00\n",
      "EG 2023-09-18 00:00:00\n",
      "HII 2022-11-23 00:00:00\n",
      "NOC 2022-11-25 00:00:00\n",
      "NOC 2023-02-24 00:00:00\n",
      "NOC 2023-05-26 00:00:00\n",
      "NOC 2023-08-25 00:00:00\n",
      "KEY 2022-11-28 00:00:00\n",
      "FDS 2022-11-29 00:00:00\n",
      "FDS 2023-02-27 00:00:00\n",
      "FDS 2023-05-30 00:00:00\n",
      "FDS 2023-08-30 00:00:00\n",
      "GS 2022-11-30 00:00:00\n",
      "GS 2023-03-01 00:00:00\n",
      "GS 2023-05-31 00:00:00\n",
      "TT 2022-12-01 00:00:00\n",
      "TT 2023-06-01 00:00:00\n",
      "UNH 2022-12-02 00:00:00\n",
      "UNH 2023-03-10 00:00:00\n",
      "UNH 2023-06-15 00:00:00\n",
      "UNH 2023-09-08 00:00:00\n",
      "CI 2022-12-05 00:00:00\n",
      "BLK 2022-12-06 00:00:00\n",
      "BLK 2023-03-06 00:00:00\n",
      "BLK 2023-06-07 00:00:00\n",
      "BLK 2023-09-07 00:00:00\n",
      "NEM 2022-12-07 00:00:00\n",
      "CME 2022-12-08 00:00:00\n",
      "CME 2022-12-27 00:00:00\n",
      "CME 2023-03-09 00:00:00\n",
      "CME 2023-06-08 00:00:00\n",
      "FDX 2022-12-09 00:00:00\n",
      "FDX 2023-06-09 00:00:00\n",
      "BBY 2022-12-12 00:00:00\n",
      "BBY 2023-03-22 00:00:00\n",
      "LRCX 2022-12-13 00:00:00\n",
      "LRCX 2023-03-14 00:00:00\n",
      "LRCX 2023-06-13 00:00:00\n",
      "LRCX 2023-09-12 00:00:00\n",
      "TMO 2022-12-14 00:00:00\n",
      "TMO 2023-06-14 00:00:00\n",
      "TMO 2023-09-14 00:00:00\n",
      "CB 2022-12-15 00:00:00\n",
      "CB 2023-03-16 00:00:00\n",
      "UNP 2022-12-16 00:00:00\n",
      "AVGO 2022-12-19 00:00:00\n",
      "AVGO 2023-03-21 00:00:00\n",
      "AVGO 2023-06-21 00:00:00\n",
      "AVGO 2023-09-20 00:00:00\n",
      "STX 2022-12-20 00:00:00\n",
      "STX 2023-06-20 00:00:00\n",
      "PM 2022-12-21 00:00:00\n",
      "PM 2023-09-26 00:00:00\n",
      "COP 2022-12-23 00:00:00\n",
      "COP 2023-02-13 00:00:00\n",
      "COP 2023-03-28 00:00:00\n",
      "COP 2023-05-15 00:00:00\n",
      "COP 2023-06-26 00:00:00\n",
      "COP 2023-08-15 00:00:00\n",
      "COP 2023-09-27 00:00:00\n",
      "XEL 2022-12-28 00:00:00\n",
      "HUM 2022-12-29 00:00:00\n",
      "HUM 2023-03-30 00:00:00\n",
      "HUM 2023-06-29 00:00:00\n",
      "HUM 2023-09-28 00:00:00\n",
      "ESS 2022-12-30 00:00:00\n",
      "CMCSA 2023-01-03 00:00:00\n",
      "CMCSA 2023-10-03 00:00:00\n",
      "CPB 2023-01-04 00:00:00\n",
      "JPM 2023-01-05 00:00:00\n",
      "JPM 2023-07-05 00:00:00\n",
      "MA 2023-01-06 00:00:00\n",
      "MA 2023-04-05 00:00:00\n",
      "MA 2023-07-06 00:00:00\n",
      "MA 2023-10-05 00:00:00\n",
      "INTU 2023-01-09 00:00:00\n",
      "INTU 2023-04-06 00:00:00\n",
      "ACN 2023-01-11 00:00:00\n",
      "ACN 2023-04-12 00:00:00\n",
      "ACN 2023-07-12 00:00:00\n",
      "MAA 2023-01-12 00:00:00\n",
      "MAA 2023-07-13 00:00:00\n",
      "MAA 2023-10-12 00:00:00\n",
      "EOG 2023-01-13 00:00:00\n",
      "EOG 2023-07-14 00:00:00\n",
      "EOG 2023-10-16 00:00:00\n",
      "DGX 2023-01-17 00:00:00\n",
      "GD 2023-01-19 00:00:00\n",
      "GD 2023-04-13 00:00:00\n",
      "COO 2023-01-20 00:00:00\n",
      "COO 2023-07-26 00:00:00\n",
      "MMC 2023-01-25 00:00:00\n",
      "MMC 2023-04-04 00:00:00\n",
      "LEN 2023-01-26 00:00:00\n",
      "OKE 2023-01-27 00:00:00\n",
      "TXN 2023-01-30 00:00:00\n",
      "TXN 2023-10-30 00:00:00\n",
      "FAST 2023-02-01 00:00:00\n",
      "FAST 2023-04-26 00:00:00\n",
      "FAST 2023-10-25 00:00:00\n",
      "COF 2023-02-03 00:00:00\n",
      "RMD 2023-02-08 00:00:00\n",
      "RMD 2023-05-10 00:00:00\n",
      "CF 2023-02-14 00:00:00\n",
      "MPC 2023-02-15 00:00:00\n",
      "MSCI 2023-02-16 00:00:00\n",
      "MSCI 2023-05-11 00:00:00\n",
      "MSCI 2023-08-10 00:00:00\n",
      "ROK 2023-02-17 00:00:00\n",
      "EFX 2023-02-21 00:00:00\n",
      "EFX 2023-05-24 00:00:00\n",
      "SNA 2023-02-22 00:00:00\n",
      "SPGI 2023-02-23 00:00:00\n",
      "SPGI 2023-05-25 00:00:00\n",
      "LMT 2023-02-28 00:00:00\n",
      "MLM 2023-03-03 00:00:00\n",
      "MLM 2023-08-31 00:00:00\n",
      "NVDA 2023-03-07 00:00:00\n",
      "NVDA 2023-09-06 00:00:00\n",
      "HD 2023-03-08 00:00:00\n",
      "LIN 2023-03-13 00:00:00\n",
      "SPY 2023-03-17 00:00:00\n",
      "SPY 2023-09-15 00:00:00\n",
      "ECL 2023-03-20 00:00:00\n",
      "ECL 2023-06-16 00:00:00\n",
      "IFF 2023-03-23 00:00:00\n",
      "IFF 2023-06-22 00:00:00\n",
      "IFF 2023-09-21 00:00:00\n",
      "EQR 2023-03-24 00:00:00\n",
      "EQR 2023-06-23 00:00:00\n",
      "EQR 2023-09-25 00:00:00\n",
      "APD 2023-03-31 00:00:00\n",
      "APD 2023-06-30 00:00:00\n",
      "APD 2023-09-29 00:00:00\n",
      "A 2023-04-03 00:00:00\n",
      "DG 2023-04-10 00:00:00\n",
      "DG 2023-07-10 00:00:00\n",
      "DG 2023-10-06 00:00:00\n",
      "HRL 2023-04-14 00:00:00\n",
      "HRL 2023-10-13 00:00:00\n",
      "PNC 2023-04-17 00:00:00\n",
      "RVTY 2023-04-20 00:00:00\n",
      "RVTY 2023-07-20 00:00:00\n",
      "RVTY 2023-10-19 00:00:00\n",
      "CAT 2023-04-21 00:00:00\n",
      "CAT 2023-07-19 00:00:00\n",
      "CAT 2023-10-20 00:00:00\n",
      "AOS 2023-04-27 00:00:00\n",
      "SYF 2023-05-01 00:00:00\n",
      "DHI 2023-05-02 00:00:00\n",
      "STZ 2023-05-03 00:00:00\n",
      "TECH 2023-05-12 00:00:00\n",
      "TECH 2023-08-17 00:00:00\n",
      "PSX 2023-05-19 00:00:00\n",
      "NDSN 2023-05-22 00:00:00\n",
      "NDSN 2023-08-21 00:00:00\n",
      "SWK 2023-06-02 00:00:00\n",
      "SWK 2023-09-01 00:00:00\n",
      "KHC 2023-06-05 00:00:00\n",
      "ODFL 2023-06-06 00:00:00\n",
      "GL 2023-07-03 00:00:00\n",
      "ROP 2023-07-07 00:00:00\n",
      "ORCL 2023-07-11 00:00:00\n",
      "F 2023-07-24 00:00:00\n",
      "J 2023-07-27 00:00:00\n",
      "J 2023-10-26 00:00:00\n",
      "VLO 2023-08-02 00:00:00\n",
      "CLX 2023-08-08 00:00:00\n",
      "CLX 2023-10-24 00:00:00\n",
      "VMC 2023-08-16 00:00:00\n",
      "WYNN 2023-08-18 00:00:00\n",
      "SBAC 2023-08-23 00:00:00\n",
      "NEE 2023-08-29 00:00:00\n",
      "PXD 2023-09-05 00:00:00\n",
      "REG 2023-09-13 00:00:00\n",
      "PKG 2023-09-22 00:00:00\n",
      "DHR 2023-10-02 00:00:00\n",
      "DHR 2023-10-11 00:00:00\n",
      "PGR 2023-10-04 00:00:00\n",
      "AMT 2023-10-10 00:00:00\n",
      "COST 2022-10-27 00:00:00\n",
      "0.07007333116879708\n",
      "0.06483378422369651\n",
      "COST 2023-02-02 00:00:00\n",
      "0.07056847663676234\n",
      "0.06617653531808729\n",
      "COST 2023-05-04 00:00:00\n",
      "0.06227980870072102\n",
      "0.058071840551393854\n",
      "COST 2023-08-24 00:00:00\n",
      "0.06713612280531822\n",
      "0.06079635846212895\n",
      "COST 2023-11-02 00:00:00\n",
      "0.05665985846064768\n",
      "0.05648374950005218\n",
      "updated predictions dictionary\n",
      "CE 2022-10-28 00:00:00\n",
      "0.07160782952950634\n",
      "0.07054624932326439\n",
      "CE 2023-07-28 00:00:00\n",
      "0.06860408960846957\n",
      "0.06758296734864068\n",
      "CE 2023-10-27 00:00:00\n",
      "0.059169441253062637\n",
      "0.06420752627658086\n",
      "updated predictions dictionary\n",
      "AON 2022-10-31 00:00:00\n",
      "0.070805185280481\n",
      "0.06926688202767153\n",
      "AON 2023-01-31 00:00:00\n",
      "0.06634539735186959\n",
      "0.06547201778700314\n",
      "AON 2023-04-28 00:00:00\n",
      "0.06550772399327397\n",
      "0.06432760633776607\n",
      "AON 2023-07-31 00:00:00\n",
      "0.06656021039935311\n",
      "0.06392806730422669\n",
      "AON 2023-10-31 00:00:00\n",
      "0.06249529916102029\n",
      "0.05881308608424752\n",
      "updated predictions dictionary\n",
      "MKTX 2022-11-01 00:00:00\n",
      "0.06922559742279043\n",
      "0.06938874673002451\n",
      "MKTX 2023-02-07 00:00:00\n",
      "0.06871209717719673\n",
      "0.06265032178453694\n",
      "MKTX 2023-05-09 00:00:00\n",
      "0.06805259250957663\n",
      "0.06468837104488105\n",
      "MKTX 2023-08-01 00:00:00\n",
      "0.06783795568672112\n",
      "0.06551456062399746\n",
      "updated predictions dictionary\n",
      "CAG 2022-11-02 00:00:00\n",
      "0.06532314331163618\n",
      "0.06663518554290837\n",
      "CAG 2023-11-01 00:00:00\n",
      "0.06294785536598022\n",
      "0.06501349300923992\n",
      "updated predictions dictionary\n",
      "NSC 2022-11-03 00:00:00\n",
      "0.06942699727911737\n",
      "0.07041979024834231\n",
      "NSC 2023-08-03 00:00:00\n",
      "0.06732646764935514\n",
      "0.06573141982247394\n",
      "updated predictions dictionary\n",
      "AMP 2022-11-04 00:00:00\n",
      "0.07076481850676845\n",
      "0.06956647829554632\n",
      "AMP 2023-02-09 00:00:00\n",
      "0.06876499571080966\n",
      "0.0664301815332542\n",
      "AMP 2023-08-04 00:00:00\n",
      "0.06555791592269887\n",
      "0.06430607712798045\n",
      "updated predictions dictionary\n",
      "AWK 2022-11-07 00:00:00\n",
      "0.07075082353871871\n",
      "0.07011730383027363\n",
      "AWK 2023-02-06 00:00:00\n",
      "0.07061453044529846\n",
      "0.07137931656791581\n",
      "AWK 2023-05-08 00:00:00\n",
      "0.06739048990642575\n",
      "0.06735267276002412\n",
      "AWK 2023-08-07 00:00:00\n",
      "0.06807534305529443\n",
      "0.06629471417396128\n",
      "updated predictions dictionary\n",
      "WST 2022-11-08 00:00:00\n",
      "0.0699204048885826\n",
      "0.06288160278020714\n",
      "WST 2023-01-24 00:00:00\n",
      "0.0693414946133051\n",
      "0.06730630773393209\n",
      "WST 2023-04-25 00:00:00\n",
      "0.06500338385546908\n",
      "0.06229382133654387\n",
      "WST 2023-07-25 00:00:00\n",
      "0.06459879752791016\n",
      "0.060008764509522555\n",
      "updated predictions dictionary\n",
      "POOL 2022-11-09 00:00:00\n",
      "0.071348367395528\n",
      "0.0710482724737866\n",
      "POOL 2023-05-16 00:00:00\n",
      "0.06532125247365456\n",
      "0.06412475813403826\n",
      "POOL 2023-08-09 00:00:00\n",
      "0.0655949830301396\n",
      "0.0669679030704954\n",
      "updated predictions dictionary\n",
      "GWW 2022-11-10 00:00:00\n",
      "0.0694450493489752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06949950354636295\n",
      "GWW 2023-02-10 00:00:00\n",
      "0.06338693374885895\n",
      "0.06942858834179855\n",
      "GWW 2023-05-05 00:00:00\n",
      "0.06265461749685389\n",
      "0.06880790223121605\n",
      "GWW 2023-08-11 00:00:00\n",
      "0.06364047674303902\n",
      "0.06840948207385601\n",
      "updated predictions dictionary\n",
      "TFX 2022-11-14 00:00:00\n",
      "0.06813737906477567\n",
      "0.06726591804986251\n",
      "TFX 2023-03-02 00:00:00\n",
      "0.07022065586714378\n",
      "0.06260939758435359\n",
      "TFX 2023-08-14 00:00:00\n",
      "0.06631794005518125\n",
      "0.06631914733324909\n",
      "updated predictions dictionary\n",
      "EQIX 2022-11-15 00:00:00\n",
      "0.0700730207076668\n",
      "0.06981854514178235\n",
      "EQIX 2023-05-23 00:00:00\n",
      "0.06422669431823082\n",
      "0.06496404015698887\n",
      "EQIX 2023-08-22 00:00:00\n",
      "0.068208088957133\n",
      "0.06674958751110584\n",
      "updated predictions dictionary\n",
      "AMGN 2022-11-16 00:00:00\n",
      "0.06429883544170796\n",
      "0.06818273207931096\n",
      "AMGN 2023-05-17 00:00:00\n",
      "0.06373952312860796\n",
      "0.06919235674686443\n",
      "updated predictions dictionary\n",
      "WHR 2022-11-17 00:00:00\n",
      "0.07093319174189996\n",
      "0.06965820669793117\n",
      "WHR 2023-05-18 00:00:00\n",
      "0.067199682016777\n",
      "0.06904711157294961\n",
      "updated predictions dictionary\n",
      "TSCO 2022-11-18 00:00:00\n",
      "0.06932852499590125\n",
      "0.06768034102542186\n",
      "updated predictions dictionary\n",
      "SWKS 2022-11-21 00:00:00\n",
      "0.07182692371969827\n",
      "0.06853917381981234\n",
      "SWKS 2023-08-28 00:00:00\n",
      "0.06411845380964115\n",
      "0.06375780843784144\n",
      "updated predictions dictionary\n",
      "EG 2022-11-22 00:00:00\n",
      "0.07034233567725767\n",
      "0.07058662733378697\n",
      "EG 2023-03-15 00:00:00\n",
      "0.0687456175265823\n",
      "0.06802807713828467\n",
      "EG 2023-09-18 00:00:00\n",
      "0.05784270072098502\n",
      "0.06402904159979762\n",
      "updated predictions dictionary\n",
      "HII 2022-11-23 00:00:00\n",
      "0.06994169496398941\n",
      "0.06829683518110734\n",
      "updated predictions dictionary\n",
      "NOC 2022-11-25 00:00:00\n",
      "0.06999783270179556\n",
      "0.06784803730551946\n",
      "NOC 2023-02-24 00:00:00\n",
      "0.06426998663007139\n",
      "0.06481046671148537\n",
      "NOC 2023-05-26 00:00:00\n",
      "0.06302863428810487\n",
      "0.063801913937875\n",
      "NOC 2023-08-25 00:00:00\n",
      "0.05976385072072762\n",
      "0.05798710102552577\n",
      "updated predictions dictionary\n",
      "KEY 2022-11-28 00:00:00\n",
      "0.07071347835250477\n",
      "0.06989345431808174\n",
      "updated predictions dictionary\n",
      "FDS 2022-11-29 00:00:00\n",
      "0.07038002376915244\n",
      "0.06758817101609192\n",
      "FDS 2023-02-27 00:00:00\n",
      "0.06898444372078663\n"
     ]
    }
   ],
   "source": [
    "def main(input_size, data_path,batch_size, n_epochs, device):\n",
    "    dataloaders, transforms = get_all_data(input_size,data_path,batch_size)\n",
    "    d_predictions = defaultdict(dict)\n",
    "    for ticker,dates in dataloaders.items():\n",
    "        #exit_loop=False\n",
    "        for date in dates:\n",
    "            print(ticker,date)\n",
    "            d_temp ={}\n",
    "            for pred_type in ['high','low']:\n",
    "                #try:\n",
    "                model = MAF(input_size=5, hidden_size=10, n_hidden=1, n_flows=5).to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "                model,optimizer,train_loss = train(model,n_epochs, dataloaders[ticker][date][pred_type]['train'],device,optimizer)\n",
    "                print(train_loss)\n",
    "                preds=transforms[ticker][date][pred_type]['transform'].inverse_transform(model.sample(device, n=1))\n",
    "                prev_close = transforms[ticker][date][pred_type]['last_close']\n",
    "                pred = np.mean(preds * prev_close + prev_close)\n",
    "                d_temp[pred_type] =pred\n",
    "                #except Exception as err:\n",
    "                #    print(pred_type,err)\n",
    "                #    exit_loop = True\n",
    "                #    break\n",
    "            #if exit_loop:\n",
    "            #    exit_loop=False\n",
    "            #    break\n",
    "            d_predictions[ticker][date] = d_temp\n",
    "        dill.dump(d_predictions,open('predictions_flow.pkd','wb'))\n",
    "        print('updated predictions dictionary')\n",
    "main(input_size=5, data_path = 'd_dfs.pkd', batch_size = 50, n_epochs=100, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
