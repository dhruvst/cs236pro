{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Compose, functional\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df, input_size, pred_type, batch_size):\n",
    " \n",
    "    if pred_type == 'low' or pred_type == 'estimated_high':\n",
    "        df = df[:-2]\n",
    "    \n",
    "    if pred_type == 'estimated_high':\n",
    "        pred_type = 'high'\n",
    "        \n",
    "    values, last_close = adjust_prev_close(df, pred_type)\n",
    "    \n",
    "    if len(values) % input_size != 0:\n",
    "        size = len(values) // input_size * input_size\n",
    "        values = values[-size:]\n",
    "    \n",
    "    transform = StandardScaler().fit(values.reshape(-1,1))\n",
    "    values = transform.transform(values.reshape(-1,1))[:,0]\n",
    "    values = torch.from_numpy(values).float()\n",
    "\n",
    "    len_dataset = (len(df)-input_size -1)//batch_size * batch_size \n",
    "\n",
    "    dataset = torch.zeros(len_dataset,input_size)\n",
    "    dataset[-1] = values[-input_size:]\n",
    "\n",
    "    for i in range(2,len_dataset+1):\n",
    "        dataset[-i] = values[-(i+input_size-1):-(i-1)]\n",
    "\n",
    "    predset = dataset[-1].unsqueeze(0)\n",
    "\n",
    "    return dataset, predset, {'transform':transform,'last_close':last_close}\n",
    "    \n",
    "def adjust_prev_close(df,pred_type):\n",
    "    last_close = df.iloc[-1]['close']\n",
    "    prev_close = df['close'].shift(fill_value=0).values[1:]\n",
    "    column = df[pred_type].values[1:]\n",
    "    result = (column - prev_close)/prev_close\n",
    "    return result, last_close\n",
    "\n",
    "def get_data(df,input_size, batch_size):\n",
    "    data, dl, tf = defaultdict(dict), defaultdict(dict), {}\n",
    "    \n",
    "    for pred_type in ['low', 'high', 'estimated_high']:\n",
    "        train_data,pred_input,transform = process_data(df, input_size, pred_type, batch_size)\n",
    "        data[pred_type]['train'] = torch.utils.data.TensorDataset(train_data)\n",
    "        data[pred_type]['pred'] = torch.utils.data.TensorDataset(pred_input)\n",
    "        dl[pred_type]['train'] = DataLoader(data[pred_type]['train'], batch_size=batch_size, shuffle=True, num_workers=0)   \n",
    "        dl[pred_type]['val'] = DataLoader(data[pred_type]['train'], batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "        tf[pred_type] = transform\n",
    "    return dl, tf\n",
    "\n",
    "def get_all_data(input_size, filepath, batch_size):\n",
    "    datasets, dataloaders, transforms = defaultdict(dict), defaultdict(dict), defaultdict(dict)\n",
    "    d_all = pd.read_pickle(filepath)\n",
    "    for ticker, dates in d_all.items():    \n",
    "        for date,df in dates.items():\n",
    "            print(ticker,date)\n",
    "            try:\n",
    "                dataloaders[ticker][date]\\\n",
    "                , transforms[ticker][date] = get_data(df,input_size, batch_size)\n",
    "            except Exception as err:\n",
    "                print(ticker,date,err)\n",
    "    return dataloaders, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MaskedLinear(nn.Linear):\n",
    "    \"\"\"Masked linear layer for MADE: takes in mask as input and masks out connections in the linear layers.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size, mask):\n",
    "        super().__init__(input_size, output_size)\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.linear(x, self.mask * self.weight, self.bias)\n",
    "\n",
    "\n",
    "class PermuteLayer(nn.Module):\n",
    "    \"\"\"Layer to permute the ordering of inputs.\n",
    "\n",
    "    Because our data is 2-D, forward() and inverse() will reorder the data in the same way.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs):\n",
    "        super(PermuteLayer, self).__init__()\n",
    "        self.perm = np.array(np.arange(0, num_inputs)[::-1])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return inputs[:, self.perm], torch.zeros(\n",
    "            inputs.size(0), 1, device=inputs.device\n",
    "        )\n",
    "\n",
    "    def inverse(self, inputs):\n",
    "        return inputs[:, self.perm], torch.zeros(\n",
    "            inputs.size(0), 1, device=inputs.device\n",
    "        )\n",
    "\n",
    "\n",
    "class MADE(nn.Module):\n",
    "    \"\"\"Masked Autoencoder for Distribution Estimation.\n",
    "    https://arxiv.org/abs/1502.03509\n",
    "\n",
    "    Uses sequential ordering as in the MAF paper.\n",
    "    Gaussian MADE to work with real-valued inputs\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, n_hidden):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        masks = self.create_masks()\n",
    "\n",
    "        # construct layers: inner, hidden(s), output\n",
    "        self.net = [MaskedLinear(self.input_size, self.hidden_size, masks[0])]\n",
    "        self.net += [nn.ReLU(inplace=True)]\n",
    "        # iterate over number of hidden layers\n",
    "        for i in range(self.n_hidden):\n",
    "            self.net += [MaskedLinear(self.hidden_size, self.hidden_size, masks[i + 1])]\n",
    "            self.net += [nn.ReLU(inplace=True)]\n",
    "        # last layer doesn't have nonlinear activation\n",
    "        self.net += [\n",
    "            MaskedLinear(self.hidden_size, self.input_size * 2, masks[-1].repeat(2, 1))\n",
    "        ]\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "\n",
    "    def create_masks(self):\n",
    "        \"\"\"\n",
    "        Creates masks for sequential (natural) ordering.\n",
    "        \"\"\"\n",
    "        masks = []\n",
    "        input_degrees = torch.arange(self.input_size)\n",
    "        degrees = [input_degrees]  # corresponds to m(k) in paper\n",
    "\n",
    "        # iterate through every hidden layer\n",
    "        for n_h in range(self.n_hidden + 1):\n",
    "            degrees += [torch.arange(self.hidden_size) % (self.input_size - 1)]\n",
    "        degrees += [input_degrees % self.input_size - 1]\n",
    "        self.m = degrees\n",
    "\n",
    "        # output layer mask\n",
    "        for (d0, d1) in zip(degrees[:-1], degrees[1:]):\n",
    "            masks += [(d1.unsqueeze(-1) >= d0.unsqueeze(0)).float()]\n",
    "\n",
    "        return masks\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Run the forward mapping (z -> x) for MAF through one MADE block.\n",
    "        :param z: Input noise of size (batch_size, self.input_size)\n",
    "        :return: (x, log_det). log_det should be 1-D (batch_dim,)\n",
    "        \"\"\"\n",
    "        x = torch.zeros_like(z)\n",
    "\n",
    "        for i in range(self.input_size):\n",
    "            h = self.net(x)\n",
    "            mu, alpha = torch.split(h, h.size(-1) // 2, dim =-1)\n",
    "            x[:,i] = mu[:,i] + z[:,i] * torch.exp(alpha[:,i])\n",
    "        log_det = -torch.sum(alpha, dim = -1)\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "        return x, log_det\n",
    "\n",
    "    def inverse(self, x):\n",
    "        \"\"\"\n",
    "        Run one inverse mapping (x -> z) for MAF through one MADE block.\n",
    "        :param x: Input data of size (batch_size, self.input_size)\n",
    "        :return: (z, log_det). log_det should be 1-D (batch_dim,)\n",
    "        \"\"\"\n",
    "        # YOUR CODE STARTS HERE\n",
    "        h =  self.net(x)\n",
    "        mu, alpha = torch.split(h, h.size(-1) // 2, dim =-1)\n",
    "        z = (x - mu) / torch.exp(alpha)\n",
    "        log_det = -torch.sum(alpha, dim = -1)\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "        return z, log_det\n",
    "\n",
    "\n",
    "class MAF(nn.Module):\n",
    "    \"\"\"\n",
    "    Masked Autoregressive Flow, using MADE layers.\n",
    "    https://arxiv.org/abs/1705.07057\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, n_hidden, n_flows):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_flows = n_flows\n",
    "        self.base_dist = torch.distributions.normal.Normal(0, 1)\n",
    "\n",
    "        # need to flip ordering of inputs for every layer\n",
    "        nf_blocks = []\n",
    "        for i in range(self.n_flows):\n",
    "            nf_blocks.append(MADE(self.input_size, self.hidden_size, self.n_hidden))\n",
    "            nf_blocks.append(PermuteLayer(self.input_size))  # permute dims\n",
    "        self.nf = nn.Sequential(*nf_blocks)\n",
    "\n",
    "    def log_probs(self, x):\n",
    "        \"\"\"\n",
    "        Obtain log-likelihood p(x) through one pass of MADE\n",
    "        :param x: Input data of size (batch_size, self.input_size)\n",
    "        :return: log_prob. This should be a Python scalar.\n",
    "        \"\"\"\n",
    "        # YOUR CODE STARTS HERE\n",
    "        sum_log_det = torch.zeros(x.size(0),1)\n",
    "        \n",
    "        for flow in self.nf:\n",
    "            x, log_det = flow.inverse(x)\n",
    "            if len(log_det.size()) ==1:\n",
    "                 log_det = log_det.unsqueeze(1)\n",
    "            sum_log_det += log_det\n",
    "\n",
    "        log_prob = torch.mean(torch.sum(self.base_dist.log_prob(x),dim=-1) + sum_log_det)\n",
    "        \n",
    "        return log_prob\n",
    "\n",
    "    def loss(self, x):\n",
    "        \"\"\"\n",
    "        Compute the loss.\n",
    "        :param x: Input data of size (batch_size, self.input_size)\n",
    "        :return: loss. This should be a Python scalar.\n",
    "        \"\"\"\n",
    "        return -self.log_probs(x)\n",
    "\n",
    "    def sample(self, device, n):\n",
    "        \"\"\"\n",
    "        Draw <n> number of samples from the model.\n",
    "        :param device: [cpu,cuda]\n",
    "        :param n: Number of samples to be drawn.\n",
    "        :return: x_sample. This should be a numpy array of size (n, self.input_size)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            x_sample = torch.randn(n, self.input_size).to(device)\n",
    "            for flow in self.nf[::-1]:\n",
    "                x_sample, log_det = flow.forward(x_sample)\n",
    "            x_sample = x_sample.view(n, self.input_size)\n",
    "            x_sample = x_sample.cpu().data.numpy()\n",
    "\n",
    "        return x_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,n_epochs,loader,device,optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    batch_idx = 0.0\n",
    "\n",
    "    for epoch in range(1,n_epochs + 1):\n",
    "        for data in loader:\n",
    "            batch_idx += 1\n",
    "\n",
    "            if isinstance(data, list):\n",
    "                data = data[0]\n",
    "\n",
    "            batch_size = len(data)\n",
    "            data = data.view(batch_size, -1)\n",
    "\n",
    "            data = data.to(device)\n",
    "\n",
    "            # run MAF\n",
    "            loss = model.loss(data)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # save stuff\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        total_loss /= batch_idx + 1\n",
    "        #print(\"Average train log-likelihood: {:.6f}\".format(-total_loss))\n",
    "\n",
    "    return model,optimizer,total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COST 2022-10-27 00:00:00\n",
      "COST 2023-02-02 00:00:00\n",
      "COST 2023-05-04 00:00:00\n",
      "COST 2023-08-24 00:00:00\n",
      "COST 2023-11-02 00:00:00\n",
      "CE 2022-10-28 00:00:00\n",
      "CE 2023-07-28 00:00:00\n",
      "CE 2023-10-27 00:00:00\n",
      "AON 2022-10-31 00:00:00\n",
      "AON 2023-01-31 00:00:00\n",
      "AON 2023-04-28 00:00:00\n",
      "AON 2023-07-31 00:00:00\n",
      "AON 2023-10-31 00:00:00\n",
      "MKTX 2022-11-01 00:00:00\n",
      "MKTX 2023-02-07 00:00:00\n",
      "MKTX 2023-05-09 00:00:00\n",
      "MKTX 2023-08-01 00:00:00\n",
      "CAG 2022-11-02 00:00:00\n",
      "CAG 2023-11-01 00:00:00\n",
      "NSC 2022-11-03 00:00:00\n",
      "NSC 2023-08-03 00:00:00\n",
      "AMP 2022-11-04 00:00:00\n",
      "AMP 2023-02-09 00:00:00\n",
      "AMP 2023-08-04 00:00:00\n",
      "AWK 2022-11-07 00:00:00\n",
      "AWK 2023-02-06 00:00:00\n",
      "AWK 2023-05-08 00:00:00\n",
      "AWK 2023-08-07 00:00:00\n",
      "WST 2022-11-08 00:00:00\n",
      "WST 2023-01-24 00:00:00\n",
      "WST 2023-04-25 00:00:00\n",
      "WST 2023-07-25 00:00:00\n",
      "POOL 2022-11-09 00:00:00\n",
      "POOL 2023-05-16 00:00:00\n",
      "POOL 2023-08-09 00:00:00\n",
      "GWW 2022-11-10 00:00:00\n",
      "GWW 2023-02-10 00:00:00\n",
      "GWW 2023-05-05 00:00:00\n",
      "GWW 2023-08-11 00:00:00\n",
      "TFX 2022-11-14 00:00:00\n",
      "TFX 2023-03-02 00:00:00\n",
      "TFX 2023-08-14 00:00:00\n",
      "EQIX 2022-11-15 00:00:00\n",
      "EQIX 2023-05-23 00:00:00\n",
      "EQIX 2023-08-22 00:00:00\n",
      "AMGN 2022-11-16 00:00:00\n",
      "AMGN 2023-05-17 00:00:00\n",
      "WHR 2022-11-17 00:00:00\n",
      "WHR 2023-05-18 00:00:00\n",
      "TSCO 2022-11-18 00:00:00\n",
      "SWKS 2022-11-21 00:00:00\n",
      "SWKS 2023-08-28 00:00:00\n",
      "EG 2022-11-22 00:00:00\n",
      "EG 2023-03-15 00:00:00\n",
      "EG 2023-09-18 00:00:00\n",
      "HII 2022-11-23 00:00:00\n",
      "NOC 2022-11-25 00:00:00\n",
      "NOC 2023-02-24 00:00:00\n",
      "NOC 2023-05-26 00:00:00\n",
      "NOC 2023-08-25 00:00:00\n",
      "KEY 2022-11-28 00:00:00\n",
      "FDS 2022-11-29 00:00:00\n",
      "FDS 2023-02-27 00:00:00\n",
      "FDS 2023-05-30 00:00:00\n",
      "FDS 2023-08-30 00:00:00\n",
      "GS 2022-11-30 00:00:00\n",
      "GS 2023-03-01 00:00:00\n",
      "GS 2023-05-31 00:00:00\n",
      "TT 2022-12-01 00:00:00\n",
      "TT 2023-06-01 00:00:00\n",
      "UNH 2022-12-02 00:00:00\n",
      "UNH 2023-03-10 00:00:00\n",
      "UNH 2023-06-15 00:00:00\n",
      "UNH 2023-09-08 00:00:00\n",
      "CI 2022-12-05 00:00:00\n",
      "BLK 2022-12-06 00:00:00\n",
      "BLK 2023-03-06 00:00:00\n",
      "BLK 2023-06-07 00:00:00\n",
      "BLK 2023-09-07 00:00:00\n",
      "NEM 2022-12-07 00:00:00\n",
      "CME 2022-12-08 00:00:00\n",
      "CME 2022-12-27 00:00:00\n",
      "CME 2023-03-09 00:00:00\n",
      "CME 2023-06-08 00:00:00\n",
      "FDX 2022-12-09 00:00:00\n",
      "FDX 2023-06-09 00:00:00\n",
      "BBY 2022-12-12 00:00:00\n",
      "BBY 2023-03-22 00:00:00\n",
      "LRCX 2022-12-13 00:00:00\n",
      "LRCX 2023-03-14 00:00:00\n",
      "LRCX 2023-06-13 00:00:00\n",
      "LRCX 2023-09-12 00:00:00\n",
      "TMO 2022-12-14 00:00:00\n",
      "TMO 2023-06-14 00:00:00\n",
      "TMO 2023-09-14 00:00:00\n",
      "CB 2022-12-15 00:00:00\n",
      "CB 2023-03-16 00:00:00\n",
      "UNP 2022-12-16 00:00:00\n",
      "AVGO 2022-12-19 00:00:00\n",
      "AVGO 2023-03-21 00:00:00\n",
      "AVGO 2023-06-21 00:00:00\n",
      "AVGO 2023-09-20 00:00:00\n",
      "STX 2022-12-20 00:00:00\n",
      "STX 2023-06-20 00:00:00\n",
      "PM 2022-12-21 00:00:00\n",
      "PM 2023-09-26 00:00:00\n",
      "COP 2022-12-23 00:00:00\n",
      "COP 2023-02-13 00:00:00\n",
      "COP 2023-03-28 00:00:00\n",
      "COP 2023-05-15 00:00:00\n",
      "COP 2023-06-26 00:00:00\n",
      "COP 2023-08-15 00:00:00\n",
      "COP 2023-09-27 00:00:00\n",
      "XEL 2022-12-28 00:00:00\n",
      "HUM 2022-12-29 00:00:00\n",
      "HUM 2023-03-30 00:00:00\n",
      "HUM 2023-06-29 00:00:00\n",
      "HUM 2023-09-28 00:00:00\n",
      "ESS 2022-12-30 00:00:00\n",
      "CMCSA 2023-01-03 00:00:00\n",
      "CMCSA 2023-10-03 00:00:00\n",
      "CPB 2023-01-04 00:00:00\n",
      "JPM 2023-01-05 00:00:00\n",
      "JPM 2023-07-05 00:00:00\n",
      "MA 2023-01-06 00:00:00\n",
      "MA 2023-04-05 00:00:00\n",
      "MA 2023-07-06 00:00:00\n",
      "MA 2023-10-05 00:00:00\n",
      "INTU 2023-01-09 00:00:00\n",
      "INTU 2023-04-06 00:00:00\n",
      "ACN 2023-01-11 00:00:00\n",
      "ACN 2023-04-12 00:00:00\n",
      "ACN 2023-07-12 00:00:00\n",
      "MAA 2023-01-12 00:00:00\n",
      "MAA 2023-07-13 00:00:00\n",
      "MAA 2023-10-12 00:00:00\n",
      "EOG 2023-01-13 00:00:00\n",
      "EOG 2023-07-14 00:00:00\n",
      "EOG 2023-10-16 00:00:00\n",
      "DGX 2023-01-17 00:00:00\n",
      "GD 2023-01-19 00:00:00\n",
      "GD 2023-04-13 00:00:00\n",
      "COO 2023-01-20 00:00:00\n",
      "COO 2023-07-26 00:00:00\n",
      "MMC 2023-01-25 00:00:00\n",
      "MMC 2023-04-04 00:00:00\n",
      "LEN 2023-01-26 00:00:00\n",
      "OKE 2023-01-27 00:00:00\n",
      "TXN 2023-01-30 00:00:00\n",
      "TXN 2023-10-30 00:00:00\n",
      "FAST 2023-02-01 00:00:00\n",
      "FAST 2023-04-26 00:00:00\n",
      "FAST 2023-10-25 00:00:00\n",
      "COF 2023-02-03 00:00:00\n",
      "RMD 2023-02-08 00:00:00\n",
      "RMD 2023-05-10 00:00:00\n",
      "CF 2023-02-14 00:00:00\n",
      "MPC 2023-02-15 00:00:00\n",
      "MSCI 2023-02-16 00:00:00\n",
      "MSCI 2023-05-11 00:00:00\n",
      "MSCI 2023-08-10 00:00:00\n",
      "ROK 2023-02-17 00:00:00\n",
      "EFX 2023-02-21 00:00:00\n",
      "EFX 2023-05-24 00:00:00\n",
      "SNA 2023-02-22 00:00:00\n",
      "SPGI 2023-02-23 00:00:00\n",
      "SPGI 2023-05-25 00:00:00\n",
      "LMT 2023-02-28 00:00:00\n",
      "MLM 2023-03-03 00:00:00\n",
      "MLM 2023-08-31 00:00:00\n",
      "NVDA 2023-03-07 00:00:00\n",
      "NVDA 2023-09-06 00:00:00\n",
      "HD 2023-03-08 00:00:00\n",
      "LIN 2023-03-13 00:00:00\n",
      "SPY 2023-03-17 00:00:00\n",
      "SPY 2023-09-15 00:00:00\n",
      "ECL 2023-03-20 00:00:00\n",
      "ECL 2023-06-16 00:00:00\n",
      "IFF 2023-03-23 00:00:00\n",
      "IFF 2023-06-22 00:00:00\n",
      "IFF 2023-09-21 00:00:00\n",
      "EQR 2023-03-24 00:00:00\n",
      "EQR 2023-06-23 00:00:00\n",
      "EQR 2023-09-25 00:00:00\n",
      "APD 2023-03-31 00:00:00\n",
      "APD 2023-06-30 00:00:00\n",
      "APD 2023-09-29 00:00:00\n",
      "A 2023-04-03 00:00:00\n",
      "DG 2023-04-10 00:00:00\n",
      "DG 2023-07-10 00:00:00\n",
      "DG 2023-10-06 00:00:00\n",
      "HRL 2023-04-14 00:00:00\n",
      "HRL 2023-10-13 00:00:00\n",
      "PNC 2023-04-17 00:00:00\n",
      "RVTY 2023-04-20 00:00:00\n",
      "RVTY 2023-07-20 00:00:00\n",
      "RVTY 2023-10-19 00:00:00\n",
      "CAT 2023-04-21 00:00:00\n",
      "CAT 2023-07-19 00:00:00\n",
      "CAT 2023-10-20 00:00:00\n",
      "AOS 2023-04-27 00:00:00\n",
      "SYF 2023-05-01 00:00:00\n",
      "DHI 2023-05-02 00:00:00\n",
      "STZ 2023-05-03 00:00:00\n",
      "TECH 2023-05-12 00:00:00\n",
      "TECH 2023-08-17 00:00:00\n",
      "PSX 2023-05-19 00:00:00\n",
      "NDSN 2023-05-22 00:00:00\n",
      "NDSN 2023-08-21 00:00:00\n",
      "SWK 2023-06-02 00:00:00\n",
      "SWK 2023-09-01 00:00:00\n",
      "KHC 2023-06-05 00:00:00\n",
      "ODFL 2023-06-06 00:00:00\n",
      "GL 2023-07-03 00:00:00\n",
      "ROP 2023-07-07 00:00:00\n",
      "ORCL 2023-07-11 00:00:00\n",
      "F 2023-07-24 00:00:00\n",
      "J 2023-07-27 00:00:00\n",
      "J 2023-10-26 00:00:00\n",
      "VLO 2023-08-02 00:00:00\n",
      "CLX 2023-08-08 00:00:00\n",
      "CLX 2023-10-24 00:00:00\n",
      "VMC 2023-08-16 00:00:00\n",
      "WYNN 2023-08-18 00:00:00\n",
      "SBAC 2023-08-23 00:00:00\n",
      "NEE 2023-08-29 00:00:00\n",
      "PXD 2023-09-05 00:00:00\n",
      "REG 2023-09-13 00:00:00\n",
      "PKG 2023-09-22 00:00:00\n",
      "DHR 2023-10-02 00:00:00\n",
      "DHR 2023-10-11 00:00:00\n",
      "PGR 2023-10-04 00:00:00\n",
      "AMT 2023-10-10 00:00:00\n",
      "COST 2022-10-27 00:00:00\n",
      "0.06729050882345346\n",
      "0.06854451207459589\n",
      "0.06921413080164854\n",
      "COST 2023-02-02 00:00:00\n",
      "0.06604721138047505\n",
      "0.06993473963118736\n",
      "0.06882168671931095\n",
      "COST 2023-05-04 00:00:00\n",
      "0.058281946703846677\n",
      "0.062467533201798334\n",
      "0.062136920396836826\n",
      "COST 2023-08-24 00:00:00\n",
      "0.061632411457549954\n",
      "0.06391914476248697\n",
      "0.06505664495624666\n",
      "COST 2023-11-02 00:00:00\n",
      "0.057487083852250306\n",
      "0.0569808465222129\n",
      "0.05617265439955394\n",
      "updated predictions dictionary\n",
      "CE 2022-10-28 00:00:00\n",
      "0.07063575902311488\n",
      "0.07218043318419122\n",
      "0.07190515994250561\n",
      "CE 2023-07-28 00:00:00\n",
      "0.0679212722449014\n",
      "0.06633459390534308\n",
      "0.06669219491915104\n",
      "CE 2023-10-27 00:00:00\n",
      "0.06481479906010178\n",
      "0.059007668554408935\n",
      "0.05867144252460541\n",
      "updated predictions dictionary\n",
      "AON 2022-10-31 00:00:00\n",
      "0.06808021480966438\n",
      "0.07158855197641029\n",
      "0.07135747433297678\n",
      "AON 2023-01-31 00:00:00\n",
      "0.06677191825639917\n",
      "0.06657038774898795\n",
      "0.06908104067543598\n",
      "AON 2023-04-28 00:00:00\n",
      "0.06524425649988642\n",
      "0.06625959715171244\n",
      "0.0661638217334977\n",
      "AON 2023-07-31 00:00:00\n",
      "0.06613991806276714\n",
      "0.06676421501236136\n",
      "0.06688233584454581\n",
      "AON 2023-10-31 00:00:00\n",
      "0.058332051046305934\n",
      "0.06252811973542488\n",
      "0.06223562673739677\n",
      "updated predictions dictionary\n",
      "MKTX 2022-11-01 00:00:00\n",
      "0.06883199610322731\n",
      "0.06994046340503143\n",
      "0.06921050153656932\n",
      "MKTX 2023-02-07 00:00:00\n",
      "0.06247353512044119\n",
      "0.06749328893684775\n",
      "0.06598414654046002\n",
      "MKTX 2023-05-09 00:00:00\n",
      "0.06460635840694505\n",
      "0.06752707676204654\n",
      "0.06472824039734827\n",
      "MKTX 2023-08-01 00:00:00\n",
      "0.06213314969232415\n",
      "0.06580834729994892\n",
      "0.06633255379837104\n",
      "updated predictions dictionary\n",
      "CAG 2022-11-02 00:00:00\n",
      "0.06510994586512592\n",
      "0.0663210653562452\n",
      "0.06565594971322929\n",
      "CAG 2023-11-01 00:00:00\n",
      "0.0658944407283656\n",
      "0.06371843827122343\n",
      "0.06365818857109792\n",
      "updated predictions dictionary\n",
      "NSC 2022-11-03 00:00:00\n",
      "0.07122537129498617\n",
      "0.06964962991012286\n",
      "0.06844900193980982\n",
      "NSC 2023-08-03 00:00:00\n",
      "0.0656747793646064\n",
      "0.06732630907798287\n",
      "0.06704552854622355\n",
      "updated predictions dictionary\n",
      "AMP 2022-11-04 00:00:00\n",
      "0.06975418550496089\n",
      "0.07078138683886497\n",
      "0.07094142740491412\n",
      "AMP 2023-02-09 00:00:00\n",
      "0.0666147447006701\n",
      "0.06887142176815202\n",
      "0.06835404785022477\n",
      "AMP 2023-08-04 00:00:00\n",
      "0.06553190740393783\n",
      "0.06486746791706438\n",
      "0.06640374816543114\n",
      "updated predictions dictionary\n",
      "AWK 2022-11-07 00:00:00\n",
      "0.07062481592673017\n",
      "0.07157780572011949\n",
      "0.07133966342670153\n",
      "AWK 2023-02-06 00:00:00\n",
      "0.07103300430703129\n",
      "0.07168602801024343\n",
      "0.07169434825398666\n",
      "AWK 2023-05-08 00:00:00\n",
      "0.06671204975492097\n",
      "0.0675825787428234\n",
      "0.06713805629801764\n",
      "AWK 2023-08-07 00:00:00\n",
      "0.06673550254339049\n",
      "0.06831980799637873\n",
      "0.06792183258365346\n",
      "updated predictions dictionary\n",
      "WST 2022-11-08 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06454422691399325\n",
      "0.06984474208393208\n",
      "0.06924707649241711\n",
      "WST 2023-01-24 00:00:00\n",
      "0.06742475016971054\n",
      "0.06924187514013101\n",
      "0.06918213010511919\n",
      "WST 2023-04-25 00:00:00\n",
      "0.06470209207654262\n",
      "0.06504807923712592\n",
      "0.06434502448386191\n",
      "WST 2023-07-25 00:00:00\n",
      "0.06068969109960541\n",
      "0.061898519989108135\n",
      "0.06219647979398496\n",
      "updated predictions dictionary\n",
      "POOL 2022-11-09 00:00:00\n",
      "0.07143405867615898\n",
      "0.07179801871333112\n",
      "0.07080151576109925\n",
      "POOL 2023-05-16 00:00:00\n",
      "0.0642984114551875\n",
      "0.06515675790556395\n",
      "0.06649082721085395\n",
      "POOL 2023-08-09 00:00:00\n",
      "0.06767920412389844\n",
      "0.06726038688906764\n",
      "0.06727916140590927\n",
      "updated predictions dictionary\n",
      "GWW 2022-11-10 00:00:00\n",
      "0.0695364353746883\n",
      "0.07004704808292876\n",
      "0.06911203871225184\n",
      "GWW 2023-02-10 00:00:00\n",
      "0.06935938649273901\n",
      "0.06828921738128042\n",
      "0.06412447630532479\n",
      "GWW 2023-05-05 00:00:00\n",
      "0.06892002533199305\n",
      "0.0623307304501454\n",
      "0.06282975838393784\n",
      "GWW 2023-08-11 00:00:00\n",
      "0.06811329206050845\n",
      "0.06119951737630778\n",
      "0.06174172677606591\n",
      "updated predictions dictionary\n",
      "TFX 2022-11-14 00:00:00\n",
      "0.06646776169192473\n",
      "0.06819576141765459\n",
      "0.06764350107741655\n",
      "TFX 2023-03-02 00:00:00\n",
      "0.06191779900189287\n",
      "0.06940780909996323\n",
      "0.06981559847321848\n",
      "TFX 2023-08-14 00:00:00\n",
      "0.06672586104358011\n",
      "0.06625847900991244\n",
      "0.0668224122550166\n",
      "updated predictions dictionary\n",
      "EQIX 2022-11-15 00:00:00\n",
      "0.06945013724235698\n",
      "0.0696051032393147\n",
      "0.06851104373962617\n",
      "EQIX 2023-05-23 00:00:00\n",
      "0.06476038610180138\n",
      "0.06490750928082228\n",
      "0.06368719135843406\n",
      "EQIX 2023-08-22 00:00:00\n",
      "0.0660894404656968\n",
      "0.06542640740523697\n",
      "0.06651388434816528\n",
      "updated predictions dictionary\n",
      "AMGN 2022-11-16 00:00:00\n",
      "0.06592262063801325\n",
      "0.06685210635100242\n",
      "0.06469446515511873\n",
      "AMGN 2023-05-17 00:00:00\n",
      "0.06889701432654348\n",
      "0.06452838950442974\n",
      "0.06684574430013411\n",
      "updated predictions dictionary\n",
      "WHR 2022-11-17 00:00:00\n",
      "0.07005432885650148\n",
      "0.07121220049407904\n",
      "0.07027370381878113\n",
      "WHR 2023-05-18 00:00:00\n",
      "0.06954846773457919\n",
      "0.0672432145518997\n",
      "0.06674585860800217\n",
      "updated predictions dictionary\n",
      "TSCO 2022-11-18 00:00:00\n",
      "0.06851361205446349\n",
      "0.06891661307492827\n",
      "0.06890693783166153\n",
      "updated predictions dictionary\n",
      "SWKS 2022-11-21 00:00:00\n",
      "0.0680701643458722\n",
      "0.07212422174703846\n",
      "0.07184969042111962\n",
      "SWKS 2023-08-28 00:00:00\n",
      "0.06461918792901783\n",
      "0.06476059405361964\n",
      "0.06564532129476679\n",
      "updated predictions dictionary\n",
      "EG 2022-11-22 00:00:00\n",
      "0.07111675791364637\n",
      "0.06907688408314146\n",
      "0.07053557430866907\n",
      "EG 2023-03-15 00:00:00\n",
      "0.06699217959443703\n",
      "0.06728502040915543\n",
      "0.06766849079228335\n",
      "EG 2023-09-18 00:00:00\n",
      "0.06391597039859806\n",
      "0.058458231077836174\n",
      "0.05961563682876735\n",
      "updated predictions dictionary\n",
      "HII 2022-11-23 00:00:00\n",
      "0.0678750731369144\n",
      "0.07037787388835348\n",
      "0.07008996479205917\n",
      "updated predictions dictionary\n",
      "NOC 2022-11-25 00:00:00\n",
      "0.06710507992586638\n",
      "0.0692968836978018\n",
      "0.06900042423202027\n",
      "NOC 2023-02-24 00:00:00\n",
      "0.06459898568680096\n",
      "0.06529718121591524\n",
      "0.06681645171881266\n",
      "NOC 2023-05-26 00:00:00\n",
      "0.06218303075360493\n",
      "0.06286633628155529\n",
      "0.06355281294064428\n",
      "NOC 2023-08-25 00:00:00\n",
      "0.05833483757787175\n",
      "0.05967212913345737\n",
      "0.05867092565585366\n",
      "updated predictions dictionary\n",
      "KEY 2022-11-28 00:00:00\n",
      "0.07060941951586173\n",
      "0.07015385547688788\n",
      "0.07139714045344724\n",
      "updated predictions dictionary\n",
      "FDS 2022-11-29 00:00:00\n",
      "0.0670816798235756\n",
      "0.07065908153848915\n",
      "0.07034253648952223\n",
      "FDS 2023-02-27 00:00:00\n",
      "0.06669200506122414\n",
      "0.06918501108795272\n",
      "0.06966548532559234\n",
      "FDS 2023-05-30 00:00:00\n",
      "0.06682579008166362\n",
      "0.06484764133044023\n",
      "0.06529686309804374\n",
      "FDS 2023-08-30 00:00:00\n",
      "0.05953509964662999\n",
      "0.06697791234243336\n",
      "0.066912371795562\n",
      "updated predictions dictionary\n",
      "GS 2022-11-30 00:00:00\n",
      "0.06707933803685423\n",
      "0.06867220068436605\n",
      "0.06783775980963537\n",
      "GS 2023-03-01 00:00:00\n",
      "0.06899193370753158\n",
      "0.06911248156578091\n",
      "0.06767867064653936\n",
      "GS 2023-05-31 00:00:00\n",
      "0.06741569047085781\n",
      "0.062315188140712206\n",
      "0.0623812915541937\n",
      "updated predictions dictionary\n",
      "TT 2022-12-01 00:00:00\n",
      "0.06896844731351263\n",
      "0.07053518988943429\n",
      "0.07234763966556952\n",
      "TT 2023-06-01 00:00:00\n",
      "0.06994958758275115\n",
      "0.06670755290832753\n",
      "0.06713832253432622\n",
      "updated predictions dictionary\n",
      "UNH 2022-12-02 00:00:00\n",
      "0.06916719729261274\n",
      "0.06809396607476878\n",
      "0.06754058372098908\n",
      "UNH 2023-03-10 00:00:00\n",
      "0.06834877329043672\n",
      "0.06758570255148372\n",
      "0.06696906079760355\n",
      "UNH 2023-06-15 00:00:00\n",
      "0.06905117110701402\n",
      "0.06067352989846123\n",
      "0.0670816004152849\n",
      "UNH 2023-09-08 00:00:00\n",
      "0.06497349212550887\n",
      "0.065473256317784\n",
      "0.06451298489020436\n",
      "updated predictions dictionary\n",
      "CI 2022-12-05 00:00:00\n",
      "0.06491587131957821\n",
      "0.06930378844019863\n",
      "0.06814374590184441\n",
      "updated predictions dictionary\n",
      "BLK 2022-12-06 00:00:00\n",
      "0.07031908105467732\n",
      "0.07069925266961978\n",
      "0.06972220026799698\n",
      "BLK 2023-03-06 00:00:00\n",
      "0.06929060622977935\n",
      "0.06805255482067389\n",
      "0.06910684829308761\n",
      "BLK 2023-06-07 00:00:00\n",
      "0.06876280281166133\n",
      "0.06353492876320256\n",
      "0.0643663591131178\n",
      "BLK 2023-09-07 00:00:00\n",
      "0.06294045222906032\n",
      "0.05345932931129065\n",
      "0.05511298889386223\n",
      "updated predictions dictionary\n",
      "NEM 2022-12-07 00:00:00\n",
      "0.06849598491943253\n",
      "0.06697820251070077\n",
      "0.06709039715042416\n",
      "updated predictions dictionary\n",
      "CME 2022-12-08 00:00:00\n",
      "0.06994356477023927\n",
      "0.06702059302732964\n",
      "0.06546913890888784\n",
      "CME 2022-12-27 00:00:00\n",
      "0.06945625630573216\n",
      "0.0643510643335128\n",
      "0.06451550051510627\n",
      "CME 2023-03-09 00:00:00\n",
      "0.06526975267700658\n",
      "0.06492063753830414\n",
      "0.06496645644571743\n",
      "CME 2023-06-08 00:00:00\n",
      "0.06660345045634634\n",
      "0.06516021832365061\n",
      "0.06608183231935967\n",
      "updated predictions dictionary\n",
      "FDX 2022-12-09 00:00:00\n",
      "0.06430959518119844\n",
      "0.06327696834142074\n",
      "0.06131963195632974\n",
      "FDX 2023-06-09 00:00:00\n",
      "0.059483983909014014\n",
      "0.058555685119934836\n",
      "0.05781322310663177\n",
      "updated predictions dictionary\n",
      "BBY 2022-12-12 00:00:00\n",
      "0.07119926991537413\n",
      "0.07144384342218847\n",
      "0.0706631107563894\n",
      "BBY 2023-03-22 00:00:00\n",
      "0.06545761136071929\n",
      "0.06432348508588413\n",
      "0.06470756363500302\n",
      "updated predictions dictionary\n",
      "LRCX 2022-12-13 00:00:00\n",
      "0.0695913791074495\n",
      "0.07128374357505972\n",
      "0.07117897793822131\n",
      "LRCX 2023-03-14 00:00:00\n",
      "0.07026809154138712\n",
      "0.06924048258916803\n",
      "0.06932633572280596\n",
      "LRCX 2023-06-13 00:00:00\n",
      "0.06600001919193625\n",
      "0.06834635792052052\n",
      "0.06872630696571282\n",
      "LRCX 2023-09-12 00:00:00\n",
      "0.05920008378492139\n",
      "0.06191804156711764\n",
      "0.06123972617239752\n",
      "updated predictions dictionary\n",
      "TMO 2022-12-14 00:00:00\n",
      "0.06910563784237639\n",
      "0.07185640722615212\n",
      "0.0728454560197749\n",
      "TMO 2023-06-14 00:00:00\n",
      "0.0677463493395278\n",
      "0.06798688605258807\n",
      "0.0683317973156299\n",
      "TMO 2023-09-14 00:00:00\n",
      "0.06341681710592753\n",
      "0.0648394352769879\n",
      "0.06455655691134389\n",
      "updated predictions dictionary\n",
      "CB 2022-12-15 00:00:00\n",
      "0.07010000103654403\n",
      "0.06865635862512924\n",
      "0.06981831605375112\n",
      "CB 2023-03-16 00:00:00\n",
      "0.06567250968271152\n",
      "0.06727903436301008\n",
      "0.06705421964002435\n",
      "updated predictions dictionary\n",
      "UNP 2022-12-16 00:00:00\n",
      "0.07030103164713532\n",
      "0.07017946778024536\n",
      "0.07101517161499175\n",
      "updated predictions dictionary\n",
      "AVGO 2022-12-19 00:00:00\n",
      "0.06941730380271927\n",
      "0.07129612852385711\n",
      "0.07084700701380292\n",
      "AVGO 2023-03-21 00:00:00\n",
      "0.06500817899929175\n",
      "0.06840883676637201\n",
      "0.06854335903018698\n",
      "AVGO 2023-06-21 00:00:00\n",
      "0.06914336232549126\n",
      "0.06548346498869032\n",
      "0.06353611933838657\n",
      "AVGO 2023-09-20 00:00:00\n",
      "0.06392810365770589\n",
      "0.061280823789614705\n",
      "0.05898839102733789\n",
      "updated predictions dictionary\n",
      "STX 2022-12-20 00:00:00\n",
      "0.0677775660897144\n",
      "0.06211285065220955\n",
      "0.06180571111317614\n",
      "STX 2023-06-20 00:00:00\n",
      "0.06575816245359586\n",
      "0.0684818053472822\n",
      "0.06890420066123945\n",
      "updated predictions dictionary\n",
      "PM 2022-12-21 00:00:00\n",
      "0.06539286125637538\n",
      "0.06883592094776515\n",
      "0.06892121248121574\n",
      "PM 2023-09-26 00:00:00\n",
      "0.06726025527965308\n",
      "0.06066211175261178\n",
      "0.06178759776199992\n",
      "updated predictions dictionary\n",
      "COP 2022-12-23 00:00:00\n",
      "0.0716552307023592\n",
      "0.07201945378123864\n",
      "0.07147347296855305\n",
      "COP 2023-02-13 00:00:00\n",
      "0.06972987036181122\n",
      "0.07027629302821813\n",
      "0.0711133596507084\n",
      "COP 2023-03-28 00:00:00\n",
      "0.06835214895579596\n",
      "0.0705025038554557\n",
      "0.06942114145065856\n",
      "COP 2023-05-15 00:00:00\n",
      "0.06351401666688486\n",
      "0.06697750099902577\n",
      "0.067018598899699\n",
      "COP 2023-06-26 00:00:00\n",
      "0.0665782172922209\n",
      "0.06716655198203615\n",
      "0.06773935484351229\n",
      "COP 2023-08-15 00:00:00\n",
      "0.06675191250587736\n",
      "0.06350880051957157\n",
      "0.06334805612995392\n",
      "COP 2023-09-27 00:00:00\n",
      "0.06672804747396895\n",
      "0.06180540220619724\n",
      "0.06465669035016618\n",
      "updated predictions dictionary\n",
      "XEL 2022-12-28 00:00:00\n",
      "0.06919963426605873\n",
      "0.0696524285192449\n",
      "0.06790287426957543\n",
      "updated predictions dictionary\n",
      "HUM 2022-12-29 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05001281040153182\n",
      "0.05695087251114911\n",
      "0.05746651061849425\n",
      "HUM 2023-03-30 00:00:00\n",
      "0.06476575398805098\n",
      "0.06402076270389505\n",
      "0.06387263236356855\n",
      "HUM 2023-06-29 00:00:00\n",
      "0.06255879687055299\n",
      "0.06389545356191953\n",
      "0.06543760399429172\n",
      "HUM 2023-09-28 00:00:00\n",
      "0.058648160562097554\n",
      "0.0649854955024848\n",
      "0.06496235896377382\n",
      "updated predictions dictionary\n",
      "ESS 2022-12-30 00:00:00\n",
      "0.06819276722086323\n",
      "0.0684483327104846\n",
      "0.06871905432846218\n",
      "updated predictions dictionary\n",
      "CMCSA 2023-01-03 00:00:00\n",
      "0.06681588893302906\n",
      "0.07082105057230158\n",
      "0.06897430218885962\n",
      "CMCSA 2023-10-03 00:00:00\n",
      "0.06623123251623385\n",
      "0.053785447920282246\n",
      "0.055086243724433506\n",
      "updated predictions dictionary\n",
      "CPB 2023-01-04 00:00:00\n",
      "0.06211913896827173\n",
      "0.06747931490485036\n",
      "0.06793655531918782\n",
      "updated predictions dictionary\n",
      "JPM 2023-01-05 00:00:00\n",
      "0.06415565085233689\n",
      "0.06603011099381517\n",
      "0.06683061385882445\n",
      "JPM 2023-07-05 00:00:00\n",
      "0.06884668831129193\n",
      "0.06812828546242099\n",
      "0.06692805058655416\n",
      "updated predictions dictionary\n",
      "MA 2023-01-06 00:00:00\n",
      "0.06521629554341535\n",
      "0.0662698795447221\n",
      "0.06568209440533647\n",
      "MA 2023-04-05 00:00:00\n",
      "0.062055634843187046\n",
      "0.06268312889810282\n",
      "0.06269701312707338\n",
      "MA 2023-07-06 00:00:00\n",
      "0.06891977085366803\n",
      "0.06973238506470045\n",
      "0.06974847173764823\n",
      "MA 2023-10-05 00:00:00\n",
      "0.06306012098495807\n",
      "0.05537719959954328\n",
      "0.05678754747450657\n",
      "updated predictions dictionary\n",
      "INTU 2023-01-09 00:00:00\n",
      "0.06865119248723707\n",
      "0.07077788226438735\n",
      "0.07100399010128608\n",
      "INTU 2023-04-06 00:00:00\n",
      "0.06451645716817571\n",
      "0.0664937994399501\n",
      "0.0668683740270585\n",
      "updated predictions dictionary\n",
      "ACN 2023-01-11 00:00:00\n",
      "0.06795526247122778\n",
      "0.0689189016056744\n",
      "0.06824335807274833\n",
      "ACN 2023-04-12 00:00:00\n",
      "0.0656762706085029\n",
      "0.06667298944181771\n",
      "0.06750458805426676\n",
      "ACN 2023-07-12 00:00:00\n",
      "0.0679467852439431\n",
      "0.06933854311091175\n",
      "0.0696264118833267\n",
      "updated predictions dictionary\n",
      "MAA 2023-01-12 00:00:00\n",
      "0.07007223652188321\n",
      "0.06719689566027559\n",
      "0.06747330223726485\n",
      "MAA 2023-07-13 00:00:00\n",
      "0.07058415153048747\n",
      "0.06969953747572405\n",
      "0.06980049320131414\n",
      "MAA 2023-10-12 00:00:00\n",
      "0.0671899481640655\n",
      "0.060523391712946524\n",
      "0.06197655017705212\n",
      "updated predictions dictionary\n",
      "EOG 2023-01-13 00:00:00\n",
      "0.07017123130044257\n",
      "0.07038043257347269\n",
      "0.06972435583572592\n",
      "EOG 2023-07-14 00:00:00\n",
      "0.06823163996322217\n",
      "0.06536171621683329\n",
      "0.06609198633056138\n",
      "EOG 2023-10-16 00:00:00\n",
      "0.06697654086265539\n",
      "0.06526830768057798\n",
      "0.06612443677809696\n",
      "updated predictions dictionary\n",
      "DGX 2023-01-17 00:00:00\n",
      "0.06961232231825236\n",
      "0.06961481918670229\n",
      "0.06907244434662445\n",
      "updated predictions dictionary\n",
      "GD 2023-01-19 00:00:00\n",
      "0.06667169769455958\n",
      "0.06816704063607686\n",
      "0.06808641201390554\n",
      "GD 2023-04-13 00:00:00\n",
      "0.06411295540069625\n",
      "0.06573916651777455\n",
      "0.06567397780873613\n",
      "updated predictions dictionary\n",
      "COO 2023-01-20 00:00:00\n",
      "0.06910676360631281\n",
      "0.06693249584005445\n",
      "0.06649596988689573\n",
      "COO 2023-07-26 00:00:00\n",
      "0.0670371614558503\n",
      "0.06267747784518929\n",
      "0.06397084490849765\n",
      "updated predictions dictionary\n",
      "MMC 2023-01-25 00:00:00\n",
      "0.07099927811098643\n",
      "0.06955665397919057\n",
      "0.07030835518711276\n",
      "MMC 2023-04-04 00:00:00\n",
      "0.06330202840737885\n",
      "0.0659259248200281\n",
      "0.06719984436908882\n",
      "updated predictions dictionary\n",
      "LEN 2023-01-26 00:00:00\n",
      "0.0679334845277754\n",
      "0.06954659416170349\n",
      "0.0683265840100873\n",
      "updated predictions dictionary\n",
      "OKE 2023-01-27 00:00:00\n",
      "0.06976635495455821\n",
      "0.0713660358274691\n",
      "0.07088662565851586\n",
      "updated predictions dictionary\n",
      "TXN 2023-01-30 00:00:00\n",
      "0.06998724183433269\n",
      "0.0706430772498821\n",
      "0.06991993717272503\n",
      "TXN 2023-10-30 00:00:00\n",
      "0.06625617768766137\n",
      "0.06158296384607611\n",
      "0.06166618426136988\n",
      "updated predictions dictionary\n",
      "FAST 2023-02-01 00:00:00\n",
      "0.07032110430317262\n",
      "0.07116397778981143\n",
      "0.07227525976823126\n",
      "FAST 2023-04-26 00:00:00\n",
      "0.06743665858496854\n",
      "0.06849726176665016\n",
      "0.0684292815743423\n",
      "FAST 2023-10-25 00:00:00\n",
      "0.06558866378223628\n",
      "0.06273454438624143\n",
      "0.06278669294840741\n",
      "updated predictions dictionary\n",
      "COF 2023-02-03 00:00:00\n",
      "0.06910113646475788\n",
      "0.07036422266881141\n",
      "0.07011743465618918\n",
      "updated predictions dictionary\n",
      "RMD 2023-02-08 00:00:00\n",
      "0.06552384571918451\n",
      "0.06960272661488309\n",
      "0.06884315487535961\n",
      "RMD 2023-05-10 00:00:00\n",
      "0.06937116583037477\n",
      "0.06592792577094442\n",
      "0.06674384661915309\n",
      "updated predictions dictionary\n",
      "CF 2023-02-14 00:00:00\n",
      "0.06383432597902829\n",
      "0.06586517722841684\n",
      "0.06538725727270998\n",
      "updated predictions dictionary\n",
      "MPC 2023-02-15 00:00:00\n",
      "0.0683178600272722\n",
      "0.07018218753990497\n",
      "0.07091924261342071\n",
      "updated predictions dictionary\n",
      "MSCI 2023-02-16 00:00:00\n",
      "0.06758616783708463\n",
      "0.07066650053444613\n",
      "0.06981481750179244\n",
      "MSCI 2023-05-11 00:00:00\n",
      "0.06665197206066241\n",
      "0.06785055367474248\n",
      "0.06948913625124384\n",
      "MSCI 2023-08-10 00:00:00\n",
      "0.06398944227356142\n",
      "0.06743559826780414\n",
      "0.06745769965364104\n",
      "updated predictions dictionary\n",
      "ROK 2023-02-17 00:00:00\n",
      "0.06671115680096976\n",
      "0.06642675213631799\n",
      "0.06820189384356978\n",
      "updated predictions dictionary\n",
      "EFX 2023-02-21 00:00:00\n",
      "0.06973950979129617\n",
      "0.06952522486364697\n",
      "0.0691543267767139\n",
      "EFX 2023-05-24 00:00:00\n",
      "0.06953359299905253\n",
      "0.06787868569580902\n",
      "0.06617932956919234\n",
      "updated predictions dictionary\n",
      "SNA 2023-02-22 00:00:00\n",
      "0.0692405307299553\n",
      "0.0654905618213385\n",
      "0.06630979081932817\n",
      "updated predictions dictionary\n",
      "SPGI 2023-02-23 00:00:00\n",
      "0.06568557721977916\n",
      "0.06912987105998154\n",
      "0.06966475557642036\n",
      "SPGI 2023-05-25 00:00:00\n",
      "0.0700687897799698\n",
      "0.06734108411840375\n",
      "0.06709948664624416\n",
      "updated predictions dictionary\n",
      "LMT 2023-02-28 00:00:00\n",
      "0.06118797402340365\n",
      "0.06359095186588892\n",
      "0.06220617566522497\n",
      "updated predictions dictionary\n",
      "MLM 2023-03-03 00:00:00\n",
      "0.06935152847025718\n",
      "0.06847058193769998\n",
      "0.0688478642483423\n",
      "MLM 2023-08-31 00:00:00\n",
      "0.0643683442204674\n",
      "0.06274464852536177\n",
      "0.06482104377232212\n",
      "updated predictions dictionary\n",
      "NVDA 2023-03-07 00:00:00\n",
      "0.06884294850364366\n",
      "0.06775270743307477\n",
      "0.06845373060838235\n",
      "NVDA 2023-09-06 00:00:00\n",
      "0.0593988867612733\n",
      "0.06136730062860653\n",
      "0.059792873572473905\n",
      "updated predictions dictionary\n",
      "HD 2023-03-08 00:00:00\n",
      "0.06776527535385994\n",
      "0.06761621534011704\n",
      "0.067324625034586\n",
      "updated predictions dictionary\n",
      "LIN 2023-03-13 00:00:00\n",
      "0.06966807135774607\n",
      "0.0696280879413496\n",
      "0.07021611663300187\n",
      "updated predictions dictionary\n",
      "SPY 2023-03-17 00:00:00\n",
      "0.06639385084844426\n",
      "0.06871759165958338\n",
      "0.06865628729042678\n",
      "SPY 2023-09-15 00:00:00\n",
      "0.0615278872151458\n",
      "0.05963632917016756\n",
      "0.05940523930174471\n",
      "updated predictions dictionary\n",
      "ECL 2023-03-20 00:00:00\n",
      "0.06856896583119278\n",
      "0.06941068354645012\n",
      "0.06978847813219356\n",
      "ECL 2023-06-16 00:00:00\n",
      "0.06825802135079947\n",
      "0.06669673061920721\n",
      "0.06889746906081834\n",
      "updated predictions dictionary\n",
      "IFF 2023-03-23 00:00:00\n",
      "0.06283935104030017\n",
      "0.06782773979171657\n",
      "0.06738197580791735\n",
      "IFF 2023-06-22 00:00:00\n",
      "0.06106200261182298\n",
      "0.07099915713657626\n",
      "0.07111207053322298\n",
      "IFF 2023-09-21 00:00:00\n",
      "0.05266846473506261\n",
      "0.0585520847264191\n",
      "0.05770519485186621\n",
      "updated predictions dictionary\n",
      "EQR 2023-03-24 00:00:00\n",
      "0.06816417641659824\n",
      "0.06989212486656836\n",
      "0.06875642720414023\n",
      "EQR 2023-06-23 00:00:00\n",
      "0.07067063705090369\n",
      "0.06860065142326985\n",
      "0.06836424391963267\n",
      "EQR 2023-09-25 00:00:00\n",
      "0.06386921242427648\n",
      "0.06326122014823234\n",
      "0.06347563497542749\n",
      "updated predictions dictionary\n",
      "APD 2023-03-31 00:00:00\n",
      "0.06746750880744795\n",
      "0.06982924957999101\n",
      "0.06893200687720603\n",
      "APD 2023-06-30 00:00:00\n",
      "0.06793051048413784\n",
      "0.06575623018649987\n",
      "0.06644829337905732\n",
      "APD 2023-09-29 00:00:00\n",
      "0.06237473674445905\n",
      "0.056758228570289566\n",
      "0.057916222772905715\n",
      "updated predictions dictionary\n",
      "A 2023-04-03 00:00:00\n",
      "0.0666124846661211\n",
      "0.06503534470283498\n",
      "0.06559749918147029\n",
      "updated predictions dictionary\n",
      "DG 2023-04-10 00:00:00\n",
      "0.04651169680339361\n",
      "0.04457778560413418\n",
      "0.045739479536115145\n",
      "DG 2023-07-10 00:00:00\n",
      "0.055065698433298754\n",
      "0.06254865655782227\n",
      "0.06294889534926965\n",
      "DG 2023-10-06 00:00:00\n",
      "0.04623087021255906\n",
      "0.06172313277692036\n",
      "0.05341188611363966\n",
      "updated predictions dictionary\n",
      "HRL 2023-04-14 00:00:00\n",
      "0.05590167904245358\n",
      "0.06764494154563597\n",
      "0.06964554558845254\n",
      "HRL 2023-10-13 00:00:00\n",
      "0.06073390735777628\n",
      "0.06403077275674447\n",
      "0.06327320086653541\n",
      "updated predictions dictionary\n",
      "PNC 2023-04-17 00:00:00\n",
      "0.06653272072099412\n",
      "0.06671381039888417\n",
      "0.06617153814215834\n",
      "updated predictions dictionary\n",
      "RVTY 2023-04-20 00:00:00\n",
      "0.06620943551841782\n",
      "0.0664352386174824\n",
      "0.06679491532863328\n",
      "RVTY 2023-07-20 00:00:00\n",
      "0.0668959990769419\n",
      "0.0654334134167071\n",
      "0.06556448864457737\n",
      "RVTY 2023-10-19 00:00:00\n",
      "0.06588373886884828\n",
      "0.06304392700655212\n",
      "0.06355502883592959\n",
      "updated predictions dictionary\n",
      "CAT 2023-04-21 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0654471986553419\n",
      "0.06678730247409827\n",
      "0.06703063686214443\n",
      "CAT 2023-07-19 00:00:00\n",
      "0.06810374281170796\n",
      "0.06684385527623944\n",
      "0.06637421517748097\n",
      "CAT 2023-10-20 00:00:00\n",
      "0.06700962700317095\n",
      "0.062451372772286795\n",
      "0.06175614779109788\n",
      "updated predictions dictionary\n",
      "AOS 2023-04-27 00:00:00\n",
      "0.068747491764685\n",
      "0.06653516550844783\n",
      "0.06688498686572887\n",
      "updated predictions dictionary\n",
      "SYF 2023-05-01 00:00:00\n",
      "0.06500609489588538\n",
      "0.06733237586911503\n",
      "0.06771291088841877\n",
      "updated predictions dictionary\n",
      "DHI 2023-05-02 00:00:00\n",
      "0.06581607542555575\n",
      "0.06571009956364454\n",
      "0.06691943911272584\n",
      "updated predictions dictionary\n",
      "STZ 2023-05-03 00:00:00\n",
      "0.06303369249316224\n",
      "0.06588515383467829\n",
      "0.06602263596579738\n",
      "updated predictions dictionary\n",
      "TECH 2023-05-12 00:00:00\n",
      "0.06922296216666601\n",
      "0.06657564118954083\n",
      "0.06797961367935591\n",
      "TECH 2023-08-17 00:00:00\n",
      "0.06747303578937638\n",
      "0.06586656376180028\n",
      "0.06688025610514942\n",
      "updated predictions dictionary\n",
      "PSX 2023-05-19 00:00:00\n",
      "0.06299813171935816\n",
      "0.06759827300807057\n",
      "0.06650857347932977\n",
      "updated predictions dictionary\n",
      "NDSN 2023-05-22 00:00:00\n",
      "0.06139459851437759\n",
      "0.06695521085507611\n",
      "0.0675095427528609\n",
      "NDSN 2023-08-21 00:00:00\n",
      "0.0574932952379315\n",
      "0.06669235740732793\n",
      "0.0669387829417681\n",
      "updated predictions dictionary\n",
      "SWK 2023-06-02 00:00:00\n",
      "0.06336120427698208\n",
      "0.06423719930399867\n",
      "0.06515720723294836\n",
      "SWK 2023-09-01 00:00:00\n",
      "0.06417675661549316\n",
      "0.06613961186146397\n",
      "0.06835890176061257\n",
      "updated predictions dictionary\n",
      "KHC 2023-06-05 00:00:00\n",
      "0.06206205083362103\n",
      "0.06614028138610778\n",
      "0.06481658124161586\n",
      "updated predictions dictionary\n",
      "ODFL 2023-06-06 00:00:00\n",
      "0.06847377348059484\n",
      "0.06581632968740592\n",
      "0.06703094624710054\n",
      "updated predictions dictionary\n",
      "GL 2023-07-03 00:00:00\n",
      "0.06545170844491295\n",
      "0.0693786440901938\n",
      "0.06898452846793461\n",
      "updated predictions dictionary\n",
      "ROP 2023-07-07 00:00:00\n",
      "0.06922117664162931\n",
      "0.06603357408127546\n",
      "0.06592602785219312\n",
      "updated predictions dictionary\n",
      "ORCL 2023-07-11 00:00:00\n",
      "0.06844556282277409\n",
      "0.06777346088678293\n",
      "0.06844836487297355\n",
      "updated predictions dictionary\n",
      "F 2023-07-24 00:00:00\n",
      "0.06637579525535584\n",
      "0.06657475090809174\n",
      "0.06855148686478862\n",
      "updated predictions dictionary\n",
      "J 2023-07-27 00:00:00\n",
      "0.06764677405529909\n",
      "0.06500302919526106\n",
      "0.06652478514264357\n",
      "J 2023-10-26 00:00:00\n",
      "0.06470762244840912\n",
      "0.060041608818018614\n",
      "0.05919704478044899\n",
      "updated predictions dictionary\n",
      "VLO 2023-08-02 00:00:00\n",
      "0.06618217462042844\n",
      "0.06591084050408015\n",
      "0.06621478354373736\n",
      "updated predictions dictionary\n",
      "CLX 2023-08-08 00:00:00\n",
      "0.06449303576557572\n",
      "0.05499790700959371\n",
      "0.058975456037306494\n",
      "CLX 2023-10-24 00:00:00\n",
      "0.06294618481356712\n",
      "0.05827606690295897\n",
      "0.055601107979453626\n",
      "updated predictions dictionary\n",
      "VMC 2023-08-16 00:00:00\n",
      "0.06306392152929305\n",
      "0.06099601020234096\n",
      "0.06304415404343776\n",
      "updated predictions dictionary\n",
      "WYNN 2023-08-18 00:00:00\n",
      "0.057507723826442946\n",
      "0.05937218145544574\n",
      "0.05881438624894981\n",
      "updated predictions dictionary\n",
      "SBAC 2023-08-23 00:00:00\n",
      "0.06626401468638721\n",
      "0.06707366379152957\n",
      "0.06643305494025573\n",
      "updated predictions dictionary\n",
      "NEE 2023-08-29 00:00:00\n",
      "0.05843330368941985\n",
      "0.06287772779632174\n",
      "0.06143060159469034\n",
      "updated predictions dictionary\n",
      "PXD 2023-09-05 00:00:00\n",
      "0.06574814432217037\n",
      "0.06393898064352499\n",
      "0.06331699792242176\n",
      "updated predictions dictionary\n",
      "REG 2023-09-13 00:00:00\n",
      "0.06432122186259635\n",
      "0.06169829598695274\n",
      "0.061889751240331\n",
      "updated predictions dictionary\n",
      "PKG 2023-09-22 00:00:00\n",
      "0.06488506028496556\n",
      "0.06084444503822405\n",
      "0.06111872197546509\n",
      "updated predictions dictionary\n",
      "DHR 2023-10-02 00:00:00\n",
      "0.06078084068125262\n",
      "0.05430590854542386\n",
      "0.06058261853138493\n",
      "DHR 2023-10-11 00:00:00\n",
      "0.05796331023917905\n",
      "0.05882580096115457\n",
      "0.06100223295080514\n",
      "updated predictions dictionary\n",
      "PGR 2023-10-04 00:00:00\n",
      "0.0579090778192785\n",
      "0.06723967019128387\n",
      "0.06591424654540248\n",
      "updated predictions dictionary\n",
      "AMT 2023-10-10 00:00:00\n",
      "0.06589278356815904\n",
      "0.05844485598068432\n",
      "0.0594017774842966\n",
      "updated predictions dictionary\n"
     ]
    }
   ],
   "source": [
    "def main(input_size, data_path,batch_size, n_epochs, device):\n",
    "    dataloaders, transforms = get_all_data(input_size,data_path,batch_size)\n",
    "    d_predictions = defaultdict(dict)\n",
    "    for ticker,dates in dataloaders.items():\n",
    "        #exit_loop=False\n",
    "        for date in dates:\n",
    "            print(ticker,date)\n",
    "            d_temp ={}\n",
    "            for pred_type in ['low', 'high', 'estimated_high']:\n",
    "                #try:\n",
    "                model = MAF(input_size, hidden_size=10, n_hidden=1, n_flows=5).to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "                model,optimizer,train_loss = train(model,n_epochs, dataloaders[ticker][date][pred_type]['train'],device,optimizer)\n",
    "                print(train_loss)\n",
    "                preds=transforms[ticker][date][pred_type]['transform'].inverse_transform(model.sample(device, n=1))\n",
    "                prev_close = transforms[ticker][date][pred_type]['last_close']\n",
    "                pred = np.mean(preds * prev_close + prev_close)\n",
    "                d_temp[pred_type] =pred\n",
    "                #except Exception as err:\n",
    "                #    print(pred_type,err)\n",
    "                #    exit_loop = True\n",
    "                #    break\n",
    "            #if exit_loop:\n",
    "            #    exit_loop=False\n",
    "            #    break\n",
    "            d_predictions[ticker][date] = d_temp\n",
    "        dill.dump(d_predictions,open('predictions_flow_est_high.pkd','wb'))\n",
    "        print('updated predictions dictionary')\n",
    "main(input_size=5, data_path = 'd_dfs.pkd', batch_size = 50, n_epochs=100, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
